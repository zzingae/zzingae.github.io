---
title: "Sampling"
tags:
  - Machine learning
use_math: true
published : false
---

# Sampling

미지의 특정 분포에 대해서, 샘플링을 무수히 많이 할수 있다면 히스토그램을 그려서 그 분포를 알수있다. 예를 들어, 전세계의 남자, 여자의 분포를 모른다고 하자. 나올수 있는 경우의 수는 남녀 2가지 이므로, 사람들을 여러번 뽑아서 남자, 여자가 나온 횟수를 비교해보면 남녀 분포를 가늠할수있다. 또한, 주어진 분포 식의 적분이 불가능한 경우, 샘플링을 통해 얻은 값으로 대략적인 평균값을 구해볼수도 있다.

이렇듯 활용도가 높은 샘플링을 어떻게 할 수 있을까?

~~만약 이미지 (100x100)의 분포를 알고 싶다고 한다면 어떨까? (100x100) 이미지의 모든 가능한 경우의 수는 256^(100x100) 가지이다. 즉, 256^(100x100) 경우가 모두 확률을 가져야 하며, 각 확률을 계산하려면 이보다 더 많은 수의 샘플이 뽑아서 히스토그램을 그려봐야 할 것이다.~~

## 딥러닝을 이용한 샘플링 기법

Generative adversarial network (GAN)은 비교적 최근 고안된 이미지 생성 기법이다. uniform 분포로 부터 샘플된 랜덤 벡터 $z$ 를 입력으로 하여 학습된 네트워크 Gererator $G$ 를 통과하면 랜덤한 이미지 $x$ 가 생성되는데, 네트워크는 $z$ 에서 $x$ 로 매핑되는 함수 $G(z)=x$ 를 학습하게 된다 (Transformation technique). 

주목할 점은, Generator의 학습을 돕기위해 Discriminator $D$ 라는 또 다른 네트워크도 함께 학습한다. $D$ 의 역할은 $G$ 가 생성한 이미지와 실제 이미지중 어떤 것이 실제 이미지 인지를 구별하는데, 만약 구별이 너무 쉽다면 $G$ 를 혼내준다 (?!). 즉, $G$ 가 실제 이미지와 구별하기 힘든 그럴싸한 생성 이미지를 만들도록 강제한다. 

$G$ 와 $D$ 간의 대립, 경쟁 (adversarial) 을 모델링한 loss 함수 덕분에, 매우 그럴싸한 생성 이미지를 만들어낸다. 만약 $D$ 가 없어서 $G$ 가 경쟁을 하지 않는다면, 이미지와 같은 고차원의 샘플을 그럴싸하게 만들기 전에 대충 만들어버리는것에 안주 할지 모른다.

## 소개

샘플링의 목적은 원하는 분포 $p$ 로 부터 샘플 $z$ 을 생성하는것이다 $z~p(z)$. 특히 관심을 갖는 $z$ 는 이미지와 같은 고차원 데이터이다. 

고차원 domain 분포 $p$ 를 알고 있다면, 우리는 특정 $z_0$ 에 대한 $p(z_0)$ 의 값을 쉽게 계산할수 있다. 하지만, $p$ 로 부터 샘플을 추출하는것은 간단하지 않다. ~~간단하지 않은 이유를 설명하기는 힘들지만, 직관적으로 고차원의 분포는 확률이 매우 낮은 부분들이 거의 대부분을 차지하고 있다. 그러한 부분들 중 하나를 확률 분포에 맞춰 공정하게 하나를 추출하는것은 매우 힘들 것이다. 또한, p(z)의 값의 출력은 scalar 값인 반면, 샘플링의 출력값은 고차원의 벡터이다. 그리고, Sampling(p)->z 로 식이 정확히 정의되지 않는다.~~

샘플링에는 몇가지 방법들이 있는데, 우선 uniform(0,1) 로 부터 pseudo-random number를 생성할수 있다고 가정한다. 또한, $\hat{p}(z) = z_p p(z)$ 이다.

## 기본적인 샘플링 방법

원하는 분포 $p$ (desired distribution) 가 복잡하여, 샘플을 직접 추출하는것이 불가능 한 경우 사용할 수 있는 방법들이다.

### Transformation technique

GAN 그래프 그림

쉽게 샘플링 할 수 있는 분포 (예를 들어, uniform)에서 추출한 샘플을 변환하여 원하는 분포 $p$ 의 샘플을 얻는 방식이다. 앞서 봤던 GAN이 이 방법으로 이미지 $x$ 를 생성한다. 즉, uniform 분포에서 샘플링한 $z$ 를 학습한 네트워크 $G$ 에 입력하고, 변환된 $G(z)=x$ 는 우리가 원하는 분포에서 얻는 샘플이 된다 $x~p(x)$.

이때, $x$ 또한 확률 랜덤 변수인데 이는 랜덤 변수 $z$ 로부터 변환됬기 때문이다.

### Rejection sampling

마찬가지로 쉽게 샘플링 할 수 있는 분포 (proposal distribution)에서 샘플을 추출한다. 하지만, 이번에는 몇몇 샘플들이 거부 (reject) 될 수 있다.

Rejection sampling의 간단한 예를 보자. 주사위를 던졌을 때의 값 1~6 을 uniform으로 샘플링 하고자 한다. 하지만 우리에게 주사위는 없고 동전 하나만 가지고 있다고 하자. 

동전을 세번 던져서 나오는 세 쌍 (예를 들어, 앞-앞-앞)의 모든 경우의 수는 8개 이다 (각 경우는 uniform이라 가정). 여기서 앞-앞-앞 과 뒤-뒤-뒤 가 나오는 경우를 거부/무시 (reject) 하면 총 6가지 경우만 남고, 6가지 각각에 값 1~6 을 매핑하면 주사위 값을 샘플링 하는 것과 같다. 

#### 방법

원하는 분포 $p(z)$ (desired distribution) 에 대해 다음을 선택한다:
- 직접 샘플링 가능한 분포 proposal distribution $q(z)$
- 모든 $z$ 에 대해 $kq(z) >= p(z)$ 인 상수 $k$

알고리즘:
1. $q$ 로 부터 $z_0$ 를 샘플링 한다.
2. uniform(0,$kq(z_0)$)로 부터 $u_0$ 를 샘플링 한다.
3. 만약 $u_0$ > $p(z_0)$ 이면 샘플 ($z_0, u_0$) 거부, 아니면 저장한다.

$u_0$ 가 거부 되는 경우를 제하면 $u$ 는 uniform(0,$p(z)$) 를 따르게 되어, ($z_0, u_0$) 쌍 $S$ 은 $p(z)$ 아래에서 uniform으로 분포한다

$S(u,z)=p(z)/u=p(z)/p(z)=1$.

따라서, $u$를 marginal 하면 $S(z)=p(z)$ 분포를 따른다. 즉, 저장된 쌍 ($z_0, u_0$) 의 $z_0$ 는 $p(z)$ 를 따르는 샘플이다. 

이 알고리즘이 가능한 이유는 $p$ 에서 샘플 추출은 어려울지라도, $p$ 에서의 특정 $z_0$ 값 $p(z_0)$ 을 계산하는것은 가능하기 때문이다.

#### 한계

거부될 가능성은 $k$ 에 따라 크게 달라진다. 따라서, $k$ 는 최대한 작으면서 $p(z)$ 보다는 모든 $z$ 에서 항상 크도록 정해야 한다.

$$\begin{aligned} p(\text { accept }) =\int{[p(z) / k q(z)]} q(z) \mathrm{d}z \\ &=\frac{1}{k} \int p(z) \mathrm{d} z
\end{aligned}$$

그러나 k를 최적으로 정한다 해도 z의 차원이 커질수록 거부 가능성은 기하급수적으로 증가한다.

예를 들어, desired distribution (p)과 proposal distribution (q) 모두 다변량 가우시안을 따른다고 하자. 

$$p~MN(0,sI1), q~MN(0,sI2)$$. 

가우시안의 꼬리 (tail)에서도 항상 크기위해 $sI2 > sI1$ 이여야 한다. 끝으로 q가 항상 p보다 크기 위해서는 q(0) > p(0) 이기만 하면 된다. 즉,

수식

k의 값이 차원에 따라 기하급수적으로 증가함에 따라, 거부 가능성 또한 기하급수적으로 증가한다. 즉, 고차원의 데이터를 샘플링하는데에는 매우 비효율적이다.

## Markov chain Monte carlo (MCMC)

MCMC는 고차원의 데이터 추출도 잘 할 수 있는 샘플링 기법이다.

rejection sampling과 같이 직접 샘플 추출 가능한 proposal distribution에서 샘플링을 한다. 하지만 차이점은 샘플링했던 값 $z_t$을 기준으로 다음 샘플 $z_{t+1}$ 을 추출 한다는 점이다.

$z~q(z_{t+1}|z_t)$

이런 방식으로 샘플링한 sequence $[z_1,z_2,..,z_t]$ 는 Markov 'chain'을 이룬다고 한다.

의문점은 이런식으로 얻은 샘플 $z~q(z|z_t)$ 들은 원하는 분포 $p$ 에서 얻은 샘플이 아니라는 것이다. 그러나 '특정 조건'을 만족하게 되면, 연속적으로 의존하는 샘플들이 얼마 후 원하는 분포 $p$ 를 따르게 된다. 아래에 자세한 조건에 대해 설명했다.

### Metropolis algorithm

Metropolis algorithm 는 MCMC 기법중 하나이다. proposal distribution에서 '후보' 샘플 $z*~q(z*|z_t)$을 추출한다. z* 는 다음의 기준에 따라 샘플로써 저장될수도 있고 버려질수도 있다.

수식

수식을 자세히 보면 새롭게 샘플링된 값 z* 에서의 desire distribution 값 p(z*) 이 클수록 저장될 가능성이 커진다 ($p(z*)>p(z_t)$ 이면 100% 저장된다). 직관적으로, 원하는 분포의 확률이 큰 곳의 샘플을 저장하는것이 자연스럽다.

만약 proposal distribution $q$ 가 다음 조건:

수식

을 만족하면, 신기하게도 샘플들이 원하는 분포 $p$ 를 따른다.

그림

가우시안 분포의 샘플들이 추출되는 동선.

### Markov chain in discrete states

Markov chain 이 이산적인 상태 공간에서 어떻게 동작하는지 살펴보자.

그림

위 그림에서 상태 공간 $S={s1,s2,s3}$ 는 총 세 개 이며, 초기 상태 $P_0$ 확률 그리고 전이 확률 $P$ 이 주어진다. 전이 확률은 Metropolis algorithm 에서 봤던 $q(z|z_t)$ 와 유사한 개념이다.

그렇다면 T번 이동을 한 후의 확률 분포 $P_T$ 는 어떻게 될까? $P_T = P P_0 P_1 .. P_{T-1}= [0,0,1]$ 에 점점 가까워진다. 즉, 특정 분포에 '수렴'한다. 또한, 다른 초기 상태 $P_0$ 에서 시작한다고 하더라도 동일한 분포에 수렴한다. 

사실 이 간단한 예에서 s3 상태는 머물 확률이 1이기 때문에, 어떤 상태에서 출발해도 s3로 빨려들어가게 되어있다. 따라서, 시간이 지나면 p(s3)=1 이 될 수 밖에 없다.

그림

위 그림의 상태 공간은 두개이며, 위 예와 같은 블랙홀은 없다. 초기 확률 분포 $P_0$ 가 주어졌을 때, 특정 분포 $P*$ 로 수렴하는 것을 확인할 수 있다. 또한 초기 확률 분포와 관계없이 $P*$ 로 수렴하게 된다. 따라서 우리는 Markov chain 이 원하는 분포로 수렴하도록 설계하면, 원하는 샘플들을 얻을 수 있을 것이다. 그렇다면 어떤 조건일때, 초기 확률 분포와 무관하게 특정 분포로 수렴하게 될까?

### Markov chain in general

Markov chain 은
- 초기 확률 분포: $P_0$
- 전이 확률 분포: $p(z_{t+1}|z_t)$

에 따라 다른 샘플 sequence를 가진다.

만약 어떤 특정 Markov chain 의 각 이동에도 $P*$ 의 분포가 바뀌지 않는다면, $P*$ 는 그 Markov chain에 대해 invariant (stationary) 분포라고 한다.

원하는 분포가 invariant 분포 이기 위한 Markov chain 의 충분 조건은, 전이 확률이 detailed balance 를 만족하는 것이다:

수식




